[
  {
    "objectID": "posts/linear-regression-r/index.html",
    "href": "posts/linear-regression-r/index.html",
    "title": "Linear Regression: From Geometry to R Implementation",
    "section": "",
    "text": "Linear regression is often introduced as an estimation problem ‚Äî coefficients, residuals, and error minimization.\nBut at its core, linear regression is geometry: projecting a response vector onto the space spanned by predictors.\nIn this article, I‚Äôll briefly explain the intuition and then implement a linear regression model in Python from a practical perspective."
  },
  {
    "objectID": "posts/linear-regression-r/index.html#the-model-setup",
    "href": "posts/linear-regression-r/index.html#the-model-setup",
    "title": "Linear Regression: From Geometry to R Implementation",
    "section": "The Model Setup",
    "text": "The Model Setup\nWe assume a linear relationship between predictors and response:\n\\[\ny = X\\beta + \\varepsilon\n\\]\nwhere\n- ( X ) is the design matrix\n- ( ) is the coefficient vector\n- ( ) is random noise\nThe objective is to find ( ) such that the squared distance between ( y ) and ( X) is minimized."
  },
  {
    "objectID": "posts/linear-regression-r/index.html#closed-form-solution",
    "href": "posts/linear-regression-r/index.html#closed-form-solution",
    "title": "Linear Regression: From Geometry to R Implementation",
    "section": "Closed-Form Solution",
    "text": "Closed-Form Solution\nThe least squares estimator is:\n\\[\n\\begin{aligned}\n\\hat{\\beta}\n&= (X^T X)^{-1} X^T y\n\\end{aligned}\n\\]\nGeometrically, this finds the orthogonal projection of ( y ) onto the column space of ( X )."
  },
  {
    "objectID": "posts/linear-regression-r/index.html#r-implementation-from-scratch",
    "href": "posts/linear-regression-r/index.html#r-implementation-from-scratch",
    "title": "Linear Regression: From Geometry to R Implementation",
    "section": "R Implementation (from Scratch)",
    "text": "R Implementation (from Scratch)\nWe start by generating synthetic data.\n\n\nCode\n# -----------------------------\n# 1. Generate synthetic data\n# -----------------------------\nset.seed(42)\n\nn &lt;- 100\nX &lt;- rnorm(n)\nbeta_true &lt;- 3\nepsilon &lt;- rnorm(n, sd = 1)\n\ny &lt;- beta_true * X + epsilon\n\ndata &lt;- data.frame(X = X, y = y)\n\n# -----------------------------\n# 2. Fit linear regression model\n# -----------------------------\nmodel &lt;- lm(y ~ X, data = data)\n\n# Model summary\nsummary(model)\n\n\n\nCall:\nlm(formula = y ~ X, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.88842 -0.50664  0.01225  0.54106  2.86240 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.08837    0.09088  -0.972    0.333    \nX            3.02716    0.08767  34.531   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9083 on 98 degrees of freedom\nMultiple R-squared:  0.9241,    Adjusted R-squared:  0.9233 \nF-statistic:  1192 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nCode\n# -----------------------------\n# 3. Extract coefficients\n# -----------------------------\ncoef(model)\n\n\n(Intercept)           X \n-0.08836678  3.02715918 \n\n\nCode\n# Intercept and slope explicitly\nintercept &lt;- coef(model)[1]\nslope &lt;- coef(model)[2]\n\n# -----------------------------\n# 4. Predictions & residuals\n# -----------------------------\ndata$y_pred &lt;- predict(model, newdata = data)\ndata$residuals &lt;- resid(model)\n\nhead(data)\n\n\n           X           y     y_pred  residuals\n1  1.3709584  5.31384072  4.0617427  1.2520981\n2 -0.5646982 -0.64934343 -1.7977980  1.1484546\n3  0.3631284  0.08617659  1.0108807 -0.9247041\n4  0.6328626  3.74706972  1.8274091  1.9196607\n5  0.4042683  0.54603156  1.1354178 -0.5893862\n6 -0.1061245 -0.21285974 -0.4096226  0.1967629\n\n\nCode\n# -----------------------------\n# 5. Visualization\n# -----------------------------\nplot(\n  data$X, data$y,\n  pch = 16,\n  col = \"steelblue\",\n  xlab = \"X\",\n  ylab = \"y\",\n  main = \"Linear Regression Fit\"\n)\n\nabline(model, col = \"red\", lwd = 2)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Digambar Dagade",
    "section": "",
    "text": "Pune, Maharashtra, India\n+91-8329290277 | dagadedigambar@gmail.com | LinkedIn"
  },
  {
    "objectID": "index.html#professional-summary",
    "href": "index.html#professional-summary",
    "title": "Digambar Dagade",
    "section": "Professional Summary",
    "text": "Professional Summary\nQuantitative Data Scientist with 2.5 years of experience in AML/FCC analytics, specializing in the end-to-end development and validation of rule-based and supervised/unsupervised machine-learning models for customer risk rating, customer segmentation for TMS, alert risk scoring (name screening and TMS), and anomaly/fraud detection.\nExperienced across multiple banking environments, including 4 months of onsite client engagement in the UAE, with strong exposure to transaction-based behavioural feature engineering, independent research, and direct client interaction.\nWell-versed in model risk management (MRM), MMS, and SR 11-7, OCC 2011-12, ECB TRIM regulatory frameworks, with a proven ability to translate analytical outputs into regulator-ready documentation and business-interpretable insights."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Digambar Dagade",
    "section": "Experience",
    "text": "Experience\n\nSolytics Partners ‚Äî Quantitative Consultant\n2023 ‚Äì Present\nCustomer Risk Rating (CRR) System\n\nDesigned and developed end-to-end customer risk rating models for 50,000+ customers using hybrid rule-based and supervised machine learning frameworks.\nEngineered demographic and transaction-based behavioural features and performed variable selection Correlation Analysis, VIF, IV, WOE, Chi-square Test, ANOVA test, using pandas and scikit-learn.\nBuilt and tuned models including Logistic Regression, Random Forest, XGBoost, and CatBoost, with hyperparameter optimization using GridSearch and Optuna.\nSelected final models based on precision, recall, PR-AUC, calibration curves, and stability metrics.\nImplemented SHAP-based explainability to interpret predictions and generate regulatory-compliant rationale for customer risk scores.\n\nCustomer Segmentation\n\nDeveloped unsupervised segmentation models using K-Means and Gaussian Mixture Models (GMM) to identify behaviour-driven customer segments for TMS.\nDetermined optimal number of clusters using Elbow Method, Silhouette Score, Davies‚ÄìBouldin Index, and Calinski‚ÄìHarabasz Index, AIC/BIC reduction.\nCombined statistical validation and business interpretability to finalize actionable and stable segments.\nBuilt a production-ready Python segmentation pipeline for 35,000+ customers and 4.5M+ transactions, including automated data validation, preprocessing, clustering, prediction, database updates, and logging.\nDeveloped interactive monitoring dashboards using Streamlit and Plotly to track segment behavior, migration, and drift.\n\nAlert Prioritization Risk Scoring Model (TMS)\n\nDesigned alert prioritization and risk-scoring models for transaction monitoring alerts to improve investigation efficiency.\nEngineered features combining customer risk attributes and transaction-level risk indicators.\nOptimized model performance using GridSearch and Optuna to balance recall‚Äìprecision trade-offs.\nIntegrated SHAP-based explanations to provide transparent, alert-level rationale for compliance analysts.\n\nModel Validation & Governance (SR 11-7 Aligned)\n\nPerformed independent validation of customer risk rating, alert prioritization, and segmentation models in alignment with SR 11-7 model risk management guidelines.\nAssessed conceptual soundness, including model design, feature relevance, assumptions, and weight justification. Conducted data and input validation, covering source assessment, completeness checks, and missing value treatment.\nEvaluated model development and selection metrics, including precision, recall, KS, IV, and WoE. Performed stability and drift analysis using PSI and CSI.\nReviewed production implementation and code, ensuring reproducibility and control effectiveness.\nEvaluated model interpretability using feature importance, SHAP, LIME, PDPs, and counterfactual analysis, and documented validation findings and monitoring recommendations."
  },
  {
    "objectID": "index.html#rd-projects",
    "href": "index.html#rd-projects",
    "title": "Digambar Dagade",
    "section": "R&D Projects",
    "text": "R&D Projects\n\nAdverse Media Screening\nDesigned LLM prompts for adverse media screening, reducing false positives by approximately 30% through contextual understanding\nAnomaly Detection Dashboard\nBuilt an AML anomaly detection framework using Isolation Forest and One-Class SVM, supported by behavioural feature engineering and LLM-generated investigative narratives\nGraph Network-Based Fraud Detection\nDeveloped graph-based AML models using network metrics and community detection to identify money laundering networks, supported by LLM-based explanations\nRAG-Powered CBUAE Guidelines Chatbot\nBuilt a compliance chatbot using Retrieval-Augmented Generation (RAG), vector embeddings, and Guardrails AI\nLLM-Based Investigation Application\nDeveloped an investigation assistant using Gemini LLM function-calling to dynamically extract customer, account, and transaction details"
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "Digambar Dagade",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nAML / FCC / Fraud\nCustomer Segmentation, Customer Risk Rating, Alert Prioritization (TMS & Name Screening), Transaction Behavioural Analytics, Suspicious Activity Detection, KYC/AML Compliance, Model Risk Management, Threshold Tuning\n\n\nMachine Learning for AML/FCC\n\nClassification: Logistic Regression, Random Forest, XGBoost, CatBoost\nAnomaly Detection: Isolation Forest, One-Class SVM\nClustering: K-Means, GMM, DBSCAN\nGraph Analytics, Feature Engineering, SHAP/LIME Explainability, ROC-AUC, PR-AUC\n\n\n\nProgramming & Tools\nPython, R, SQL, Pandas, scikit-learn, XGBoost, CatBoost, MLflow, Streamlit, Power BI, AWS (SageMaker, Bedrock), Plotly, FastAPI\n\n\nGenerative AI\nRAG Architecture, Vector Embeddings, Prompt Engineering, Agentic AI"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Digambar Dagade",
    "section": "Education",
    "text": "Education\n\nM.Sc. in Statistics\nShivaji University, Kolhapur (2021 ‚Äì 2023)\nCGPA: 9.5 / 10\nKey Coursework:\nLinear Algebra, Statistical Inference, Regression Analysis, Optimization Research, Data Mining, Time Series Analysis, Design of Experiments, Generalized Linear Models, Multivariate Analysis"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "Below are selected articles where I write about statistics, machine learning, AML/FCC analytics, and the geometry behind models."
  },
  {
    "objectID": "articles.html#featured-posts",
    "href": "articles.html#featured-posts",
    "title": "Articles",
    "section": "üìò Featured Posts",
    "text": "üìò Featured Posts\n\nThe Journey from Linear Regression to Logistic Regression\nUnderstanding regression as a projection problem in linear algebra.\n\n\nThe Linear Regression Implementation in R\nUnderstanding regression as a projection problem in linear algebra."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#where-everything-starts-linearity",
    "href": "posts/linear-regression-geometry/index.html#where-everything-starts-linearity",
    "title": "From Linear Regression to Logistic Regression",
    "section": "Where Everything Starts: Linearity",
    "text": "Where Everything Starts: Linearity\nLinear regression begins with a deceptively simple assumption about the data-generating process:\n\\[\n\\mathbb{E}(Y \\mid X) = X\\beta\n\\]\nThis assumption is powerful because it aligns perfectly with the nature of the response variable.\nIn classical linear regression, (Y) can take any value on the real line. There are no natural boundaries. Predicting (-5), (3.2), or (10^6) does not violate any rules.\nThe linear predictor \\(X\\beta\\) also lives on the real line, so the model is internally consistent.\nLinearity works not because it is convenient, but because the mathematics of the response allows it."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#a-fundamental-shift-binary-outcomes",
    "href": "posts/linear-regression-geometry/index.html#a-fundamental-shift-binary-outcomes",
    "title": "From Linear Regression to Logistic Regression",
    "section": "A Fundamental Shift: Binary Outcomes",
    "text": "A Fundamental Shift: Binary Outcomes\nNow imagine a different kind of problem. Instead of predicting a continuous quantity, we are predicting an event.\n\nDid a transaction turn suspicious?\nDid a customer default?\nDid a user click?\n\nIn all these cases, the response variable is binary:\n\\[\nY \\in \\{0, 1\\}\n\\]\nA natural probabilistic assumption is:\n\\[\nY \\sim \\text{Bernoulli}(p)\n\\]\nHere, the randomness in the data is fully described by a single quantity: the probability \\(p = P(Y = 1 \\mid X)\\).\nFor a Bernoulli variable, the conditional expectation simplifies beautifully:\n\\[\n\\begin{aligned}\n\\mathbb{E}(Y \\mid X)\n&= 1 \\cdot P(Y=1 \\mid X) \\\\\n&\\quad + 0 \\cdot P(Y=0 \\mid X)\n= p\n\\end{aligned}\n\\]\nAt this point, the expectation is no longer an abstract mean, it is the probability itself."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#why-linear-regression-fails-here",
    "href": "posts/linear-regression-geometry/index.html#why-linear-regression-fails-here",
    "title": "From Linear Regression to Logistic Regression",
    "section": "Why Linear Regression Fails Here",
    "text": "Why Linear Regression Fails Here\nIf we try to carry over the linear regression logic unchanged, we would write:\n\\[\np = X\\beta\n\\]\nThis looks innocent, but it immediately breaks something fundamental.\nThe probability \\(p\\) must lie between 0 and 1.\nThe linear predictor \\(X\\beta\\) has no such restriction, it can be negative, arbitrarily large, or anything in between.\nThis mismatch is not a technical nuisance; it is a conceptual failure.\nA model that predicts probabilities outside \\([0,1]\\) is not merely inaccurate, it is invalid.\nThe issue is not with linearity itself. The issue is where we are trying to apply linearity."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#rethinking-the-target-from-probability-to-odds",
    "href": "posts/linear-regression-geometry/index.html#rethinking-the-target-from-probability-to-odds",
    "title": "From Linear Regression to Logistic Regression",
    "section": "Rethinking the Target: From Probability to Odds",
    "text": "Rethinking the Target: From Probability to Odds\nInstead of forcing a linear structure onto the probability, we step back and change perspective.\nRather than modeling (p) directly, we model the odds:\n\\[\n\\text{odds} = \\frac{p}{1 - p}\n\\]\nOdds answer a different question:\n‚ÄúHow many times more likely is success than failure?‚Äù\nUnlike probabilities, odds are not constrained to lie between 0 and 1. They range from:\n\\[\n0 \\quad \\text{to} \\quad +\\infty\n\\]\nThis is progress. We have removed the upper bound. But odds are still restricted, they cannot be negative.\nSo linearity still does not quite fit."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#the-key-insight-log-odds-restore-linearity",
    "href": "posts/linear-regression-geometry/index.html#the-key-insight-log-odds-restore-linearity",
    "title": "From Linear Regression to Logistic Regression",
    "section": "The Key Insight: Log-Odds Restore Linearity",
    "text": "The Key Insight: Log-Odds Restore Linearity\nTo fully remove constraints, we apply one final transformation and take the logarithm of the odds:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right)\n\\]\nThis quantity, known as the log-odds or logit, can take any real value.\nNow, and only now, do both sides of the equation live in the same mathematical space.\nThis allows us to write:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) = X\\beta\n\\]This single equation defines logistic regression.\nNotice what has happened conceptually. We did not abandon the linear model.\nWe preserved linearity by applying it to a transformed version of the response.\nLinearity survives, but on the log-odds scale."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#interpretation-what-the-coefficients-really-mean",
    "href": "posts/linear-regression-geometry/index.html#interpretation-what-the-coefficients-really-mean",
    "title": "From Linear Regression to Logistic Regression",
    "section": "Interpretation: What the Coefficients Really Mean",
    "text": "Interpretation: What the Coefficients Really Mean\nThis formulation gives logistic regression its distinctive interpretation.\nA one-unit increase in a predictor \\(X_j\\) increases the log-odds by \\(\\beta_j\\). Exponentiating both sides reveals the effect on odds:\n\\[\n\\text{odds} \\;\\rightarrow\\; \\text{odds} \\times e^{\\beta_j}\n\\]\nSo coefficients no longer represent additive changes in the response. They represent multiplicative changes in odds.\nThis is why logistic regression coefficients are often reported as odds ratios rather than raw estimates."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#completing-the-journey-from-log-odds-back-to-probability",
    "href": "posts/linear-regression-geometry/index.html#completing-the-journey-from-log-odds-back-to-probability",
    "title": "From Linear Regression to Logistic Regression",
    "section": "Completing the Journey: From Log-Odds Back to Probability",
    "text": "Completing the Journey: From Log-Odds Back to Probability\nSolving the log-odds equation for (p) gives:\n\\[\np = \\frac{1}{1 + e^{-X\\beta}}\n\\]\nThis is the sigmoid function. It smoothly maps any real-valued input into a valid probability between 0 and 1.\nAt this point, the journey is complete.\nLinear regression and logistic regression are not separate ideas.\nLogistic regression is what linear regression becomes when probability theory forces us to respect its boundaries.\nThe model remains linear in spirit, only the scale changes."
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "TEST",
    "section": "",
    "text": "THIS IS A QUARTO TEST PAGE"
  }
]