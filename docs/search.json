[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Digambar Dagade",
    "section": "",
    "text": "üìç Pune, Maharashtra, India\nüìû +91-8329290277 | ‚úâÔ∏è dagadedigambar@gmail.com | LinkedIn"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#professional-summary",
    "href": "index.html#professional-summary",
    "title": "Digambar Dagade",
    "section": "Professional Summary",
    "text": "Professional Summary\nQuantitative Data Scientist with 2.5 years of experience in AML/FCC analytics, specializing in the end-to-end development and validation of rule-based and supervised/unsupervised machine-learning models for customer risk rating, customer segmentation for TMS, alert risk scoring (name screening and TMS), and anomaly/fraud detection. Experienced across multiple banking environments, including 4 months of onsite client engagement in the UAE, with strong exposure to transaction-based behavioural feature engineering, independent research, and direct client interaction.\nWell-versed in model risk management (MRM), MMS, and SR 11-7, OCC 2011-12, ECB TRIM regulatory frameworks, with a proven ability to translate analytical outputs into regulator-ready documentation and business-interpretable insights."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Digambar Dagade",
    "section": "Experience",
    "text": "Experience\n\nSolytics Partners ‚Äî Quantitative Consultant\n2023 ‚Äì Present\nCustomer Risk Rating (CRR) System\n\nDesigned and developed end-to-end customer risk rating models for 50,000+ customers using hybrid rule-based and supervised machine learning frameworks.\nEngineered demographic and transaction-based behavioural features and performed variable selection Correlation Analysis, VIF, IV, WOE, Chi-square Test, ANOVA test, using pandas and scikit-learn.\nBuilt and tuned models including Logistic Regression, Random Forest, XGBoost, and CatBoost, with hyperparameter optimization using GridSearch and Optuna.\nSelected final models based on precision, recall, PR-AUC, calibration curves, and stability metrics.\nImplemented SHAP-based explainability to interpret predictions and generate regulatory-compliant rationale for customer risk scores.\n\nCustomer Segmentation\n\nDeveloped unsupervised segmentation models using K-Means and Gaussian Mixture Models (GMM) to identify behaviour-driven customer segments for TMS.\nDetermined optimal number of clusters using Elbow Method, Silhouette Score, Davies‚ÄìBouldin Index, and Calinski‚ÄìHarabasz Index, AIC/BIC reduction.\nCombined statistical validation and business interpretability to finalize actionable and stable segments.\nBuilt a production-ready Python segmentation pipeline for 35,000+ customers and 4.5M+ transactions, including automated data validation, preprocessing, clustering, prediction, database updates, and logging.\nDeveloped interactive monitoring dashboards using Streamlit and Plotly to track segment behavior, migration, and drift.\n\nAlert Prioritization Risk Scoring Model (TMS)\n\nDesigned alert prioritization and risk-scoring models for transaction monitoring alerts to improve investigation efficiency.\nEngineered features combining customer risk attributes and transaction-level risk indicators.\nOptimized model performance using GridSearch and Optuna to balance recall‚Äìprecision trade-offs.\nIntegrated SHAP-based explanations to provide transparent, alert-level rationale for compliance analysts.\n\nModel Validation & Governance (SR 11-7 Aligned)\n\nPerformed independent validation of customer risk rating, alert prioritization, and segmentation models in alignment with SR 11-7 model risk management guidelines.\nAssessed conceptual soundness, including model design, feature relevance, assumptions, and weight justification. Conducted data and input validation, covering source assessment, completeness checks, and missing value treatment.\nEvaluated model development and selection metrics, including precision, recall, KS, IV, and WoE. Performed stability and drift analysis using PSI and CSI.\nReviewed production implementation and code, ensuring reproducibility and control effectiveness.\nEvaluated model interpretability using feature importance, SHAP, LIME, PDPs, and counterfactual analysis, and documented validation findings and monitoring recommendations."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Digambar Dagade",
    "section": "Skills",
    "text": "Skills\n\nProgramming: Python, R, SQL\nAnalytics: Statistics, Machine Learning, Anomaly Detection\nAML/FCC: TMS, Screening, Risk Rating, Model Validation\nTools: Git, Quarto, Power BI, Excel"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Digambar Dagade",
    "section": "Education",
    "text": "Education\n\nM.Sc. in Statistics\nShivaji University, Kolhapur (2021 ‚Äì 2023)\nCGPA: 9.5 / 10\nKey Coursework:\nLinear Algebra, Statistical Inference, Regression Analysis, Optimization Research, Data Mining, Time Series Analysis, Design of Experiments, Generalized Linear Models, Multivariate Analysis"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Digambar Dagade",
    "section": "Interests",
    "text": "Interests\n\nWriting about statistics & data science\nTrekking across forts in Maharashtra\nParagliding, zip-lining & adventure sports\nMilitary & Sci-Fi web series"
  },
  {
    "objectID": "index.html#rd-projects",
    "href": "index.html#rd-projects",
    "title": "Digambar Dagade",
    "section": "R&D Projects",
    "text": "R&D Projects\n\nAdverse Media Screening\nDesigned LLM prompts for adverse media screening, reducing false positives by approximately 30% through contextual understanding\nAnomaly Detection Dashboard\nBuilt an AML anomaly detection framework using Isolation Forest and One-Class SVM, supported by behavioural feature engineering and LLM-generated investigative narratives\nGraph Network-Based Fraud Detection\nDeveloped graph-based AML models using network metrics and community detection to identify money laundering networks, supported by LLM-based explanations\nRAG-Powered CBUAE Guidelines Chatbot\nBuilt a compliance chatbot using Retrieval-Augmented Generation (RAG), vector embeddings, and Guardrails AI\nLLM-Based Investigation Application\nDeveloped an investigation assistant using Gemini LLM function-calling to dynamically extract customer, account, and transaction details"
  },
  {
    "objectID": "index.html#technical-skills",
    "href": "index.html#technical-skills",
    "title": "Digambar Dagade",
    "section": "Technical Skills",
    "text": "Technical Skills\n\nAML / FCC / Fraud\nCustomer Segmentation, Customer Risk Rating, Alert Prioritization (TMS & Name Screening), Transaction Behavioural Analytics, Suspicious Activity Detection, KYC/AML Compliance, Model Risk Management, Threshold Tuning\n\n\nMachine Learning for AML/FCC\n\nClassification: Logistic Regression, Random Forest, XGBoost, CatBoost\nAnomaly Detection: Isolation Forest, One-Class SVM\nClustering: K-Means, GMM, DBSCAN\nGraph Analytics, Feature Engineering, SHAP/LIME Explainability, ROC-AUC, PR-AUC\n\n\n\nProgramming & Tools\nPython, R, SQL, Pandas, scikit-learn, XGBoost, CatBoost, MLflow, Streamlit, Power BI, AWS (SageMaker, Bedrock), Plotly, FastAPI\n\n\nGenerative AI\nRAG Architecture, Vector Embeddings, Prompt Engineering, Agentic AI"
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html",
    "href": "posts/linear-regression-geometry/index.html",
    "title": "The Journey from Linear Regression to Logistic Regression",
    "section": "",
    "text": "In linear regression, everything starts with a simple but powerful assumption about the conditional expectation of the response variable:\n\\[\n\\mathbb{E}(Y \\mid X) = X\\beta\n\\]\nThis assumption works beautifully because the response variable (Y) is allowed to take any value on the real line. Whether the model predicts (-10), (2.5), or (1000), there is no mathematical restriction being violated. The linear combination (X) naturally lives in (), and so does the expected value of (Y).\nIn this setting, linearity is not just convenient‚Äîit is valid."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#the-predictor-space",
    "href": "posts/linear-regression-geometry/index.html#the-predictor-space",
    "title": "Linear Regression Is Geometry, Not Estimation",
    "section": "The Predictor Space",
    "text": "The Predictor Space\nLet the design matrix be:\n\\[\nX = [x_1, x_2, \\dots, x_p]\n\\]\nEach column represents a direction in space. All linear combinations of these columns span a subspace called the column space of (X)."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#projection-and-residuals",
    "href": "posts/linear-regression-geometry/index.html#projection-and-residuals",
    "title": "Linear Regression Is Geometry, Not Estimation",
    "section": "Projection and Residuals",
    "text": "Projection and Residuals\nThe fitted regression values are obtained by orthogonally projecting the response vector onto the column space of the design matrix.\nThe perpendicular distance between the response vector and this subspace is minimized ‚Äî this distance is what we call the residual."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#why-this-perspective-matters",
    "href": "posts/linear-regression-geometry/index.html#why-this-perspective-matters",
    "title": "Linear Regression Is Geometry, Not Estimation",
    "section": "Why This Perspective Matters",
    "text": "Why This Perspective Matters\n\nExplains why least squares works\n\nClarifies multicollinearity\n\nConnects regression directly to linear algebra\n\nOnce you see regression this way, the formula becomes secondary."
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "Below are selected articles where I write about statistics, machine learning, AML/FCC analytics, and the geometry behind models."
  },
  {
    "objectID": "articles.html#featured-posts",
    "href": "articles.html#featured-posts",
    "title": "Articles",
    "section": "üìò Featured Posts",
    "text": "üìò Featured Posts\n\nThe Journey from Linear Regression to Logistic Regression\nUnderstanding regression as a projection problem in linear algebra."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#the-response-vector",
    "href": "posts/linear-regression-geometry/index.html#the-response-vector",
    "title": "Linear Regression Is Geometry, Not Estimation",
    "section": "The Response Vector",
    "text": "The Response Vector\nThe response variable is a vector:\n\\[\ny \\in \\mathbb{R}^n\n\\]\nIn most real datasets, (y) does not lie in the column space of (X)."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#projection-and-least-squares",
    "href": "posts/linear-regression-geometry/index.html#projection-and-least-squares",
    "title": "Linear Regression Is Geometry, Not Estimation",
    "section": "Projection and Least Squares",
    "text": "Projection and Least Squares\nLinear regression finds the projection of (y) onto the column space of (X):\n\\[\n\\hat{y} = X\\hat{\\beta}\n\\]\nwhere the coefficients are obtained by solving:\n\\[\n\\hat{\\beta} = (X^TX)^{-1}X^Ty\n\\]\nGeometrically, this minimizes the squared distance:\n\\[\n\\|y - X\\hat{\\beta}\\|^2\n\\]"
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#residual-as-perpendicular-distance",
    "href": "posts/linear-regression-geometry/index.html#residual-as-perpendicular-distance",
    "title": "Linear Regression Is Geometry, Not Estimation",
    "section": "Residual as Perpendicular Distance",
    "text": "Residual as Perpendicular Distance\nThe residual vector is:\n\\[\nr = y - \\hat{y}\n\\]\nA key geometric property:\n\\[\nX^T r = 0\n\\]\nThis means the residual is orthogonal to the predictor subspace."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#setting-the-stage-linear-regression",
    "href": "posts/linear-regression-geometry/index.html#setting-the-stage-linear-regression",
    "title": "The Journey from Linear Regression to Logistic Regression",
    "section": "",
    "text": "In linear regression, everything starts with a simple but powerful assumption about the conditional expectation of the response variable:\n\\[\n\\mathbb{E}(Y \\mid X) = X\\beta\n\\]\nThis assumption works beautifully because the response variable (Y) is allowed to take any value on the real line. Whether the model predicts (-10), (2.5), or (1000), there is no mathematical restriction being violated. The linear combination (X) naturally lives in (), and so does the expected value of (Y).\nIn this setting, linearity is not just convenient‚Äîit is valid."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#when-the-response-becomes-binary",
    "href": "posts/linear-regression-geometry/index.html#when-the-response-becomes-binary",
    "title": "The Journey from Linear Regression to Logistic Regression",
    "section": "When the Response Becomes Binary",
    "text": "When the Response Becomes Binary\nNow consider a fundamentally different problem. Suppose the response variable is binary:\n\\[\nY \\in \\{0, 1\\}\n\\]\nThis is common in classification problems such as fraud detection, default prediction, or medical diagnosis. In such cases, we usually assume:\n\\[\nY \\sim \\text{Bernoulli}(p)\n\\]\nwhere (p = P(Y = 1 X)).\nFor a Bernoulli random variable, the conditional expectation takes a very specific form:\n\\[\n\\mathbb{E}(Y \\mid X) = 1 \\cdot P(Y=1 \\mid X) + 0 \\cdot P(Y=0 \\mid X) = p\n\\]\nSo far, everything is still mathematically sound. The expectation of a binary variable is simply the probability of success."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#the-core-problem-with-linearity",
    "href": "posts/linear-regression-geometry/index.html#the-core-problem-with-linearity",
    "title": "The Journey from Linear Regression to Logistic Regression",
    "section": "The Core Problem with Linearity",
    "text": "The Core Problem with Linearity\nIf we now try to reuse the linear regression idea directly, we would write:\n\\[\np = X\\beta\n\\]\nThis is where the real struggle begins.\nThe probability (p) must lie between 0 and 1, by definition. However, the linear predictor (X) can take any value on the real line, from (-) to (+).\nThis mismatch is not a minor technical issue‚Äîit violates the basic rules of probability. A model that can predict probabilities less than 0 or greater than 1 is not meaningful.\nAt this point, linear regression breaks down‚Äînot because linearity is wrong, but because we are applying it to the wrong quantity."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#changing-what-we-model-from-probability-to-odds",
    "href": "posts/linear-regression-geometry/index.html#changing-what-we-model-from-probability-to-odds",
    "title": "The Journey from Linear Regression to Logistic Regression",
    "section": "Changing What We Model: From Probability to Odds",
    "text": "Changing What We Model: From Probability to Odds\nTo fix the problem, we take a conceptual step back and ask a different question.\nInstead of modeling the probability (p) directly, we model the odds:\n\\[\n\\text{odds} = \\frac{p}{1 - p}\n\\]\nOdds behave differently from probabilities. They are no longer constrained to the interval ([0,1]). Instead, they range from:\n\\[\n0 \\quad \\text{to} \\quad +\\infty\n\\]\nThis is an improvement, but the odds are still restricted‚Äîthey only live on the positive side of the real line. We are closer, but not there yet."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#the-final-step-log-odds-and-linearity",
    "href": "posts/linear-regression-geometry/index.html#the-final-step-log-odds-and-linearity",
    "title": "The Journey from Linear Regression to Logistic Regression",
    "section": "The Final Step: Log-Odds and Linearity",
    "text": "The Final Step: Log-Odds and Linearity\nTo fully remove the constraint, we take one final transformation and apply the natural logarithm to the odds:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right)\n\\]\nThis quantity, called the log-odds or logit, can take any real value. Now the left-hand side lives in the same space as a linear predictor.\nAt this point, linearity becomes valid again:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) = X\\beta\n\\]\nThis is the defining equation of logistic regression.\nWe have not abandoned linearity. Instead, we have relocated it‚Äîfrom the probability scale to the log-odds scale."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#interpretation-why-logistic-regression-is-so-powerful",
    "href": "posts/linear-regression-geometry/index.html#interpretation-why-logistic-regression-is-so-powerful",
    "title": "The Journey from Linear Regression to Logistic Regression",
    "section": "Interpretation: Why Logistic Regression Is So Powerful",
    "text": "Interpretation: Why Logistic Regression Is So Powerful\nThis formulation gives logistic regression its elegant interpretation.\nA one-unit increase in a predictor (X_j) increases the log-odds by (_j). Exponentiating both sides shows what happens to the odds:\n\\[\n\\text{odds} \\;\\rightarrow\\; \\text{odds} \\times e^{\\beta_j}\n\\]\nSo each coefficient represents a multiplicative change in odds, not an additive change in probability. This is why logistic regression coefficients are often interpreted using odds ratios."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#completing-the-journey-the-sigmoid-function",
    "href": "posts/linear-regression-geometry/index.html#completing-the-journey-the-sigmoid-function",
    "title": "The Journey from Linear Regression to Logistic Regression",
    "section": "Completing the Journey: The Sigmoid Function",
    "text": "Completing the Journey: The Sigmoid Function\nSolving the log-odds equation for (p) gives:\n\\[\np = \\frac{1}{1 + e^{-X\\beta}}\n\\]\nThis is the familiar sigmoid function. It smoothly maps any real-valued input to a valid probability between 0 and 1.\nWith this final step, the journey is complete.\nLinear regression and logistic regression are not unrelated models. Logistic regression is what linear regression becomes when the response variable forces us to respect the geometry and constraints of probability.\nLinearity survives‚Äîbut only after transformation."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#where-everything-starts-linearity",
    "href": "posts/linear-regression-geometry/index.html#where-everything-starts-linearity",
    "title": "From Linear Regression to Logistic Regression",
    "section": "Where Everything Starts: Linearity",
    "text": "Where Everything Starts: Linearity\nLinear regression begins with a deceptively simple assumption about the data-generating process:\n\\[\n\\mathbb{E}(Y \\mid X) = X\\beta\n\\]\nThis assumption is powerful because it aligns perfectly with the nature of the response variable.\nIn classical linear regression, (Y) can take any value on the real line. There are no natural boundaries. Predicting (-5), (3.2), or (10^6) does not violate any rules.\nThe linear predictor \\(X\\beta\\) also lives on the real line, so the model is internally consistent.\nLinearity works not because it is convenient, but because the mathematics of the response allows it."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#a-fundamental-shift-binary-outcomes",
    "href": "posts/linear-regression-geometry/index.html#a-fundamental-shift-binary-outcomes",
    "title": "From Linear Regression to Logistic Regression",
    "section": "A Fundamental Shift: Binary Outcomes",
    "text": "A Fundamental Shift: Binary Outcomes\nNow imagine a different kind of problem. Instead of predicting a continuous quantity, we are predicting an event.\nDid a transaction turn suspicious?\nDid a customer default?\nDid a user click?\nIn all these cases, the response variable is binary:\n\\[\nY \\in \\{0, 1\\}\n\\]\nA natural probabilistic assumption is:\n\\[\nY \\sim \\text{Bernoulli}(p)\n\\]\nHere, the randomness in the data is fully described by a single quantity: the probability \\((p = P(Y = 1 \\mid X))\\).\nFor a Bernoulli variable, the conditional expectation simplifies beautifully:\n\\[\n\\mathbb{E}(Y \\mid X)\n= 1 \\cdot P(Y=1 \\mid X) + 0 \\cdot P(Y=0 \\mid X)\n= p\n\\]\nAt this point, the expectation is no longer an abstract mean‚Äîit is the probability itself."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#why-linear-regression-fails-here",
    "href": "posts/linear-regression-geometry/index.html#why-linear-regression-fails-here",
    "title": "From Linear Regression to Logistic Regression",
    "section": "Why Linear Regression Fails Here",
    "text": "Why Linear Regression Fails Here\nIf we try to carry over the linear regression logic unchanged, we would write:\n\\[\np = X\\beta\n\\]\nThis looks innocent, but it immediately breaks something fundamental.\nThe probability \\(p\\) must lie between 0 and 1.\n\nThe linear predictor \\(X\\beta\\) has no such restriction, it can be negative, arbitrarily large, or anything in between.\nThis mismatch is not a technical nuisance; it is a conceptual failure.\nA model that predicts probabilities outside \\([0,1]\\) is not merely inaccurate, it is invalid.\nThe issue is not with linearity itself.\nThe issue is where we are trying to apply linearity."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#rethinking-the-target-from-probability-to-odds",
    "href": "posts/linear-regression-geometry/index.html#rethinking-the-target-from-probability-to-odds",
    "title": "From Linear Regression to Logistic Regression",
    "section": "Rethinking the Target: From Probability to Odds",
    "text": "Rethinking the Target: From Probability to Odds\nInstead of forcing a linear structure onto the probability, we step back and change perspective.\nRather than modeling (p) directly, we model the odds:\n\\[\n\\text{odds} = \\frac{p}{1 - p}\n\\]\nOdds answer a different question:\n‚ÄúHow many times more likely is success than failure?‚Äù\nUnlike probabilities, odds are not constrained to lie between 0 and 1. They range from:\n\\[\n0 \\quad \\text{to} \\quad +\\infty\n\\]\nThis is progress. We have removed the upper bound. But odds are still restricted‚Äîthey cannot be negative.\nSo linearity still does not quite fit."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#the-key-insight-log-odds-restore-linearity",
    "href": "posts/linear-regression-geometry/index.html#the-key-insight-log-odds-restore-linearity",
    "title": "From Linear Regression to Logistic Regression",
    "section": "The Key Insight: Log-Odds Restore Linearity",
    "text": "The Key Insight: Log-Odds Restore Linearity\nTo fully remove constraints, we apply one final transformation and take the logarithm of the odds:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right)\n\\]\nThis quantity, known as the log-odds or logit, can take any real value.\nNow, and only now, do both sides of the equation live in the same mathematical space.\nThis allows us to write:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) = X\\beta\n\\]\nThis single equation defines logistic regression.\nNotice what has happened conceptually. We did not abandon the linear model.\nWe preserved linearity by applying it to a transformed version of the response.\nLinearity survives‚Äîbut on the log-odds scale."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#interpretation-what-the-coefficients-really-mean",
    "href": "posts/linear-regression-geometry/index.html#interpretation-what-the-coefficients-really-mean",
    "title": "From Linear Regression to Logistic Regression",
    "section": "Interpretation: What the Coefficients Really Mean",
    "text": "Interpretation: What the Coefficients Really Mean\nThis formulation gives logistic regression its distinctive interpretation.\nA one-unit increase in a predictor \\(X_j\\) increases the log-odds by (_j). Exponentiating both sides reveals the effect on odds:\n\\[\n\\text{odds} \\;\\rightarrow\\; \\text{odds} \\times e^{\\beta_j}\n\\]\nSo coefficients no longer represent additive changes in the response. They represent multiplicative changes in odds.\nThis is why logistic regression coefficients are often reported as odds ratios rather than raw estimates."
  },
  {
    "objectID": "posts/linear-regression-geometry/index.html#completing-the-journey-from-log-odds-back-to-probability",
    "href": "posts/linear-regression-geometry/index.html#completing-the-journey-from-log-odds-back-to-probability",
    "title": "From Linear Regression to Logistic Regression",
    "section": "Completing the Journey: From Log-Odds Back to Probability",
    "text": "Completing the Journey: From Log-Odds Back to Probability\nSolving the log-odds equation for (p) gives:\n\\[\np = \\frac{1}{1 + e^{-X\\beta}}\n\\]\nThis is the sigmoid function. It smoothly maps any real-valued input into a valid probability between 0 and 1.\nAt this point, the journey is complete.\nLinear regression and logistic regression are not separate ideas.\nLogistic regression is what linear regression becomes when probability theory forces us to respect its boundaries.\nThe model remains linear in spirit, only the scale changes."
  }
]